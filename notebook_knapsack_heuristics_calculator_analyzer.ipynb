{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R34s4tKYPj4"
      },
      "source": [
        "# Knapsack Heuristics Calculator and Analyzer\n",
        "\n",
        "Complete system for evaluating greedy heuristics on 0/1 Knapsack Problem instances.\n",
        "\n",
        "**Features:**\n",
        "- 8 greedy heuristics with tie-breaking mechanisms\n",
        "- Dynamic programming optimal solver\n",
        "- Parallel processing support\n",
        "- Comprehensive statistical analysis\n",
        "- Automated report generation\n",
        "- Pydantic validation for data integrity\n",
        "- Unit testing suite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag59RSQCYPj5"
      },
      "source": [
        "## 1. Imports and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TYkXA4bZYPj6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Protocol\n",
        "from abc import ABC, abstractmethod\n",
        "import logging\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from functools import partial\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pytz\n",
        "import psutil\n",
        "\n",
        "from numba import jit\n",
        "from pydantic import BaseModel, Field, field_validator, ConfigDict, model_validator\n",
        "from pydantic_settings import BaseSettings\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "try:\n",
        "    import cpuinfo\n",
        "    HAS_CPUINFO = True\n",
        "except ImportError:\n",
        "    HAS_CPUINFO = False\n",
        "    logger.debug(\"cpuinfo not available, will use psutil only for hardware info\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0ys4GeEYPj7"
      },
      "source": [
        "## 2. Configuration Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Ez2XpZPhYPj7"
      },
      "outputs": [],
      "source": [
        "class ExperimentConfig(BaseSettings):\n",
        "    model_config = ConfigDict(env_prefix='KP_')\n",
        "\n",
        "    capacity: int = Field(\n",
        "        default=64,\n",
        "        gt=0,\n",
        "        description=\"Knapsack capacity for all instances\"\n",
        "    )\n",
        "\n",
        "    data_dir: str = Field(\n",
        "        default='/content/drive/MyDrive/PlataZarateBayliss-Instances_extracted_csv',\n",
        "        description=\"Directory containing instance CSV files\"\n",
        "    )\n",
        "\n",
        "    output_dir: str = Field(\n",
        "        default='/content',\n",
        "        description=\"Directory for output files\"\n",
        "    )\n",
        "\n",
        "    pattern: str = Field(\n",
        "        default='*.csv',\n",
        "        description=\"File pattern for instance files\"\n",
        "    )\n",
        "\n",
        "    compute_optimal: bool = Field(\n",
        "        default=True,\n",
        "        description=\"Whether to compute optimal solutions via DP\"\n",
        "    )\n",
        "\n",
        "    timezone: str = Field(\n",
        "        default='America/Monterrey',\n",
        "        description=\"Timezone for timestamps\"\n",
        "    )\n",
        "\n",
        "    log_level: str = Field(\n",
        "        default='INFO',\n",
        "        description=\"Logging level (DEBUG, INFO, WARNING, ERROR)\"\n",
        "    )\n",
        "\n",
        "    random_seed: Optional[int] = Field(\n",
        "        default=None,\n",
        "        description=\"Random seed for reproducibility (future use)\"\n",
        "    )\n",
        "\n",
        "    max_workers: Optional[int] = Field(\n",
        "        default=None,\n",
        "        description=\"Max workers for parallel processing (None = auto-detect from CPU count)\"\n",
        "    )\n",
        "\n",
        "    enable_parallel: bool = Field(\n",
        "        default=True,\n",
        "        description=\"Enable parallel processing for multiple instances\"\n",
        "    )\n",
        "\n",
        "    @field_validator('max_workers')\n",
        "    @classmethod\n",
        "    def validate_max_workers(cls, v: Optional[int]) -> Optional[int]:\n",
        "        if v is not None:\n",
        "            cpu_count = psutil.cpu_count(logical=True)\n",
        "            if v > cpu_count * 2:\n",
        "                logger.warning(\n",
        "                    f\"max_workers={v} exceeds 2x CPU count ({cpu_count}). \"\n",
        "                    f\"This may cause performance degradation.\"\n",
        "                )\n",
        "            if v < 1:\n",
        "                raise ValueError(\"max_workers must be >= 1\")\n",
        "        return v\n",
        "\n",
        "    @field_validator('data_dir', 'output_dir')\n",
        "    @classmethod\n",
        "    def validate_directory(cls, v: str) -> str:\n",
        "        path = Path(v)\n",
        "        if not path.exists():\n",
        "            logger.warning(f\"Directory {v} does not exist, will attempt to create\")\n",
        "        return v\n",
        "\n",
        "    @field_validator('log_level')\n",
        "    @classmethod\n",
        "    def validate_log_level(cls, v: str) -> str:\n",
        "        valid_levels = {'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'}\n",
        "        v_upper = v.upper()\n",
        "        if v_upper not in valid_levels:\n",
        "            raise ValueError(f\"Log level must be one of {valid_levels}\")\n",
        "        return v_upper\n",
        "\n",
        "    def setup_logging(self):\n",
        "        logging.getLogger().setLevel(self.log_level)\n",
        "        logger.setLevel(self.log_level)\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return {\n",
        "            'capacity': self.capacity,\n",
        "            'data_dir': self.data_dir,\n",
        "            'output_dir': self.output_dir,\n",
        "            'pattern': self.pattern,\n",
        "            'compute_optimal': self.compute_optimal,\n",
        "            'timezone': self.timezone,\n",
        "            'log_level': self.log_level,\n",
        "            'random_seed': self.random_seed,\n",
        "            'max_workers': self.max_workers,\n",
        "            'python_version': f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\",\n",
        "            'numpy_version': np.__version__,\n",
        "            'pandas_version': pd.__version__,\n",
        "            'timestamp': datetime.now(pytz.timezone(self.timezone)).isoformat()\n",
        "        }\n",
        "\n",
        "    def save_to_file(self, filepath: str):\n",
        "        import json\n",
        "        config_dict = self.to_dict()\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(config_dict, f, indent=2)\n",
        "        logger.info(f\"Configuration saved to {filepath}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_file(cls, filepath: str) -> 'ExperimentConfig':\n",
        "        import json\n",
        "        with open(filepath, 'r') as f:\n",
        "            config_dict = json.load(f)\n",
        "        return cls(**{k: v for k, v in config_dict.items() if k in cls.model_fields})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQs6GV-cYPj7"
      },
      "source": [
        "## 3. Data Models with Pydantic Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2H6eL8_EYPj8"
      },
      "outputs": [],
      "source": [
        "class KnapsackInstance(BaseModel):\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "\n",
        "    profits: np.ndarray\n",
        "    weights: np.ndarray\n",
        "    capacity: int = Field(gt=0, description=\"Knapsack capacity must be positive\")\n",
        "    name: str = Field(min_length=1, description=\"Instance name\")\n",
        "\n",
        "    @field_validator('profits', 'weights')\n",
        "    @classmethod\n",
        "    def validate_arrays(cls, v: np.ndarray) -> np.ndarray:\n",
        "        if not isinstance(v, np.ndarray):\n",
        "            raise ValueError(\"Must be numpy array\")\n",
        "        if v.dtype != np.int64:\n",
        "            v = v.astype(np.int64)\n",
        "        if len(v) == 0:\n",
        "            raise ValueError(\"Array cannot be empty\")\n",
        "        if np.any(v <= 0):\n",
        "            raise ValueError(f\"All values must be positive, found {np.sum(v <= 0)} non-positive values\")\n",
        "        return v\n",
        "\n",
        "    @model_validator(mode='after')\n",
        "    def validate_arrays_match(self):\n",
        "        if len(self.profits) != len(self.weights):\n",
        "            raise ValueError(\n",
        "                f\"Mismatched array lengths: profits={len(self.profits)}, weights={len(self.weights)}\"\n",
        "            )\n",
        "\n",
        "        items_that_fit = np.sum(self.weights <= self.capacity)\n",
        "        if items_that_fit == 0:\n",
        "            raise ValueError(\n",
        "                f\"No items can fit in capacity {self.capacity}: \"\n",
        "                f\"minimum weight is {np.min(self.weights)}, all {len(self.weights)} items exceed capacity\"\n",
        "            )\n",
        "\n",
        "        max_sum = np.sum(self.profits)\n",
        "        if max_sum > np.iinfo(np.int64).max // 2:\n",
        "            raise ValueError(\n",
        "                f\"Risk of overflow: sum of profits {max_sum} exceeds safe threshold \"\n",
        "                f\"{np.iinfo(np.int64).max // 2}\"\n",
        "            )\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def num_items(self) -> int:\n",
        "        return len(self.profits)\n",
        "\n",
        "    @classmethod\n",
        "    def from_csv(cls, filepath: str, capacity: int) -> 'KnapsackInstance':\n",
        "        if not Path(filepath).exists():\n",
        "            raise FileNotFoundError(f\"File not found: {filepath}\")\n",
        "\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        required_columns = {'profit', 'weight'}\n",
        "        if not required_columns.issubset(df.columns):\n",
        "            raise ValueError(f\"CSV must contain columns: {required_columns}, found: {df.columns.tolist()}\")\n",
        "\n",
        "        return cls(\n",
        "            profits=df['profit'].values.astype(np.int64),\n",
        "            weights=df['weight'].values.astype(np.int64),\n",
        "            capacity=capacity,\n",
        "            name=Path(filepath).stem\n",
        "        )\n",
        "\n",
        "\n",
        "class HeuristicResult(BaseModel):\n",
        "    heuristic_name: str = Field(min_length=1)\n",
        "    solution_value: int = Field(ge=0, description=\"Solution value must be non-negative\")\n",
        "    execution_time: float = Field(ge=0, description=\"Execution time must be non-negative\")\n",
        "\n",
        "    @field_validator('execution_time')\n",
        "    @classmethod\n",
        "    def validate_execution_time(cls, v: float) -> float:\n",
        "        if v > 3600:\n",
        "            logger.warning(f\"Execution time {v}s exceeds 1 hour, possible timing error\")\n",
        "        return v\n",
        "\n",
        "\n",
        "class InstanceResult(BaseModel):\n",
        "    instance_name: str = Field(min_length=1)\n",
        "    capacity: int = Field(gt=0)\n",
        "    heuristic_results: Dict[str, HeuristicResult]\n",
        "    optimal_value: Optional[int] = Field(default=None, ge=0)\n",
        "    optimal_time: Optional[float] = Field(default=None, ge=0)\n",
        "\n",
        "    @model_validator(mode='after')\n",
        "    def validate_results(self):\n",
        "        if self.optimal_value is not None:\n",
        "            for name, result in self.heuristic_results.items():\n",
        "                if result.solution_value > self.optimal_value:\n",
        "                    raise ValueError(\n",
        "                        f\"CRITICAL BUG DETECTED: Heuristic {name} returned {result.solution_value} > \"\n",
        "                        f\"optimal {self.optimal_value}. This indicates a bug in the implementation.\"\n",
        "                    )\n",
        "        return self\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        result = {\n",
        "            'instance': self.instance_name,\n",
        "            'capacity': self.capacity\n",
        "        }\n",
        "        for name, heur_result in self.heuristic_results.items():\n",
        "            result[name] = heur_result.solution_value\n",
        "        if self.optimal_value is not None:\n",
        "            result['optimal'] = self.optimal_value\n",
        "        return result\n",
        "\n",
        "    def to_json(self) -> str:\n",
        "        return self.model_dump_json(indent=2)\n",
        "\n",
        "    @classmethod\n",
        "    def from_json(cls, json_str: str) -> 'InstanceResult':\n",
        "        return cls.model_validate_json(json_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4cV-fh4YPj9"
      },
      "source": [
        "## 4. Numba-Optimized Core Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MRyrDp-5YPj9"
      },
      "outputs": [],
      "source": [
        "@jit(nopython=True)\n",
        "def _greedy_selection(profits: np.ndarray, weights: np.ndarray,\n",
        "                      capacity: np.int64, indices: np.ndarray) -> np.int64:\n",
        "    \"\"\"\n",
        "    Greedy knapsack selection with pre-determined item order.\n",
        "    \"\"\"\n",
        "    total_profit = 0\n",
        "    total_weight = 0\n",
        "    for i in indices:\n",
        "        if total_weight + weights[i] <= capacity:\n",
        "            total_weight += weights[i]\n",
        "            total_profit += profits[i]\n",
        "    return total_profit\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _compute_ratios(profits: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
        "    n = len(profits)\n",
        "    ratios = np.empty(n, dtype=np.float64)\n",
        "    for i in range(n):\n",
        "        if weights[i] > 0:\n",
        "            ratios[i] = profits[i] / weights[i]\n",
        "        else:\n",
        "            ratios[i] = 1e308 if profits[i] > 0 else 0.0\n",
        "    return ratios\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _knapsack_dp(profits: np.ndarray, weights: np.ndarray, capacity: np.int64) -> np.int64:\n",
        "    n: np.int64 = len(profits)\n",
        "    prev = np.zeros(capacity + 1, dtype=np.int64)\n",
        "    curr = np.zeros(capacity + 1, dtype=np.int64)\n",
        "\n",
        "    for i in range(n):\n",
        "        for w in range(capacity + 1):\n",
        "            if weights[i] <= w:\n",
        "                curr[w] = max(prev[w], prev[w - weights[i]] + profits[i])\n",
        "            else:\n",
        "                curr[w] = prev[w]\n",
        "        prev, curr = curr, prev\n",
        "\n",
        "    return prev[capacity]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt0kDogkYPj9"
      },
      "source": [
        "## 5. Heuristic Implementations (Strategy Pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "csQ7qZ20YPj9"
      },
      "outputs": [],
      "source": [
        "class Heuristic(ABC):\n",
        "    @abstractmethod\n",
        "    def solve(self, instance: KnapsackInstance) -> int:\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def name(self) -> str:\n",
        "        pass\n",
        "\n",
        "\n",
        "class DefaultHeuristic(Heuristic):\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return 'default'\n",
        "\n",
        "    def solve(self, instance: KnapsackInstance) -> int:\n",
        "        indices = np.arange(len(instance.profits), dtype=np.int64)\n",
        "        return _greedy_selection(instance.profits, instance.weights, instance.capacity, indices)\n",
        "\n",
        "\n",
        "class MaxProfitHeuristic(Heuristic):\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return 'max_profit'\n",
        "\n",
        "    def solve(self, instance: KnapsackInstance) -> int:\n",
        "        indices = np.argsort(instance.profits)[::-1].astype(np.int64)\n",
        "        return _greedy_selection(instance.profits, instance.weights, instance.capacity, indices)\n",
        "\n",
        "\n",
        "class MaxProfitPerWeightHeuristic(Heuristic):\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return 'max_profit_per_weight'\n",
        "\n",
        "    def solve(self, instance: KnapsackInstance) -> int:\n",
        "        ratios = _compute_ratios(instance.profits, instance.weights)\n",
        "        indices = np.argsort(ratios)[::-1].astype(np.int64)\n",
        "        return _greedy_selection(instance.profits, instance.weights, instance.capacity, indices)\n",
        "\n",
        "\n",
        "class MinWeightHeuristic(Heuristic):\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return 'min_weight'\n",
        "\n",
        "    def solve(self, instance: KnapsackInstance) -> int:\n",
        "        indices = np.argsort(instance.weights).astype(np.int64)\n",
        "        return _greedy_selection(instance.profits, instance.weights, instance.capacity, indices)\n",
        "\n",
        "\n",
        "class MaxProfitPerWeightTiebreakProfitHeuristic(Heuristic):\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return 'max_profit_per_weight_tiebreak_profit'\n",
        "\n",
        "    def solve(self, instance: KnapsackInstance) -> int:\n",
        "        ratios = _compute_ratios(instance.profits, instance.weights)\n",
        "        indices = np.lexsort((instance.profits, ratios))[::-1].astype(np.int64)\n",
        "        return _greedy_selection(instance.profits, instance.weights, instance.capacity, indices)\n",
        "\n",
        "\n",
        "class MaxProfitPerWeightTiebreakWeightHeuristic(Heuristic):\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return 'max_profit_per_weight_tiebreak_weight'\n",
        "\n",
        "    def solve(self, instance: KnapsackInstance) -> int:\n",
        "        ratios = _compute_ratios(instance.profits, instance.weights)\n",
        "        indices = np.lexsort((instance.weights, -ratios)).astype(np.int64)\n",
        "        return _greedy_selection(instance.profits, instance.weights, instance.capacity, indices)\n",
        "\n",
        "\n",
        "class MaxProfitTiebreakWeightHeuristic(Heuristic):\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return 'max_profit_tiebreak_weight'\n",
        "\n",
        "    def solve(self, instance: KnapsackInstance) -> int:\n",
        "        indices = np.lexsort((instance.weights, -instance.profits)).astype(np.int64)\n",
        "        return _greedy_selection(instance.profits, instance.weights, instance.capacity, indices)\n",
        "\n",
        "\n",
        "class MinWeightTiebreakProfitHeuristic(Heuristic):\n",
        "    @property\n",
        "    def name(self) -> str:\n",
        "        return 'min_weight_tiebreak_profit'\n",
        "\n",
        "    def solve(self, instance: KnapsackInstance) -> int:\n",
        "        indices = np.lexsort((-instance.profits, instance.weights)).astype(np.int64)\n",
        "        return _greedy_selection(instance.profits, instance.weights, instance.capacity, indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R68XuB9rYPj-"
      },
      "source": [
        "## 6. Optimal Solver and Heuristic Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sHQfzmtMYPj-"
      },
      "outputs": [],
      "source": [
        "class OptimalSolver:\n",
        "    def solve(self, instance: KnapsackInstance) -> int:\n",
        "        return _knapsack_dp(instance.profits, instance.weights, instance.capacity)\n",
        "\n",
        "    def solve_with_time(self, instance: KnapsackInstance) -> Tuple[int, float]:\n",
        "        start_time = time.perf_counter()\n",
        "        result = _knapsack_dp(instance.profits, instance.weights, instance.capacity)\n",
        "        execution_time = time.perf_counter() - start_time\n",
        "        return result, execution_time\n",
        "\n",
        "\n",
        "class HeuristicRegistry:\n",
        "    def __init__(self):\n",
        "        self._heuristics: Dict[str, Heuristic] = {}\n",
        "        self._register_defaults()\n",
        "\n",
        "    def _register_defaults(self):\n",
        "        default_heuristics = [\n",
        "            DefaultHeuristic(),\n",
        "            MaxProfitHeuristic(),\n",
        "            MaxProfitPerWeightHeuristic(),\n",
        "            MinWeightHeuristic(),\n",
        "            MaxProfitPerWeightTiebreakProfitHeuristic(),\n",
        "            MaxProfitPerWeightTiebreakWeightHeuristic(),\n",
        "            MaxProfitTiebreakWeightHeuristic(),\n",
        "            MinWeightTiebreakProfitHeuristic()\n",
        "        ]\n",
        "        for heuristic in default_heuristics:\n",
        "            self.register(heuristic)\n",
        "\n",
        "    def register(self, heuristic: Heuristic):\n",
        "        self._heuristics[heuristic.name] = heuristic\n",
        "\n",
        "    def get(self, name: str) -> Heuristic:\n",
        "        return self._heuristics[name]\n",
        "\n",
        "    def get_all(self) -> Dict[str, Heuristic]:\n",
        "        return self._heuristics.copy()\n",
        "\n",
        "    def get_names(self) -> List[str]:\n",
        "        return list(self._heuristics.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa8j1ZCUYPj-"
      },
      "source": [
        "## 7. Experiment Executor with Parallel Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YB8dL0UGYPj-"
      },
      "outputs": [],
      "source": [
        "def _process_instance_wrapper(filepath: str, capacity: int, compute_optimal: bool,\n",
        "                              registry: HeuristicRegistry, solver: OptimalSolver) -> Optional[InstanceResult]:\n",
        "    try:\n",
        "        instance = KnapsackInstance.from_csv(filepath, capacity)\n",
        "\n",
        "        heuristic_results = {}\n",
        "        for name, heuristic in registry.get_all().items():\n",
        "            start_time = time.perf_counter()\n",
        "            solution_value = heuristic.solve(instance)\n",
        "            execution_time = time.perf_counter() - start_time\n",
        "\n",
        "            heuristic_results[name] = HeuristicResult(\n",
        "                heuristic_name=name,\n",
        "                solution_value=solution_value,\n",
        "                execution_time=execution_time\n",
        "            )\n",
        "\n",
        "        optimal_value = None\n",
        "        optimal_time = None\n",
        "        if compute_optimal:\n",
        "            optimal_value, optimal_time = solver.solve_with_time(instance)\n",
        "\n",
        "        return InstanceResult(\n",
        "            instance_name=instance.name,\n",
        "            capacity=instance.capacity,\n",
        "            heuristic_results=heuristic_results,\n",
        "            optimal_value=optimal_value,\n",
        "            optimal_time=optimal_time\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to process {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "class ExperimentExecutor:\n",
        "    def __init__(self, registry: HeuristicRegistry, optimal_solver: OptimalSolver):\n",
        "        self.registry = registry\n",
        "        self.optimal_solver = optimal_solver\n",
        "        self.logger = logging.getLogger(__name__ + '.ExperimentExecutor')\n",
        "\n",
        "    def execute_single_instance(self, instance: KnapsackInstance,\n",
        "                                compute_optimal: bool = False) -> InstanceResult:\n",
        "        heuristic_results = {}\n",
        "\n",
        "        for name, heuristic in self.registry.get_all().items():\n",
        "            start_time = time.perf_counter()\n",
        "            solution_value = heuristic.solve(instance)\n",
        "            execution_time = time.perf_counter() - start_time\n",
        "\n",
        "            heuristic_results[name] = HeuristicResult(\n",
        "                heuristic_name=name,\n",
        "                solution_value=solution_value,\n",
        "                execution_time=execution_time\n",
        "            )\n",
        "\n",
        "        optimal_value = None\n",
        "        optimal_time = None\n",
        "        if compute_optimal:\n",
        "            optimal_value, optimal_time = self.optimal_solver.solve_with_time(instance)\n",
        "\n",
        "        return InstanceResult(\n",
        "            instance_name=instance.name,\n",
        "            capacity=instance.capacity,\n",
        "            heuristic_results=heuristic_results,\n",
        "            optimal_value=optimal_value,\n",
        "            optimal_time=optimal_time\n",
        "        )\n",
        "\n",
        "    def execute_directory(self, directory: str, capacity: int,\n",
        "                         pattern: str = '*.csv',\n",
        "                         compute_optimal: bool = False,\n",
        "                         parallel: bool = False,\n",
        "                         max_workers: Optional[int] = None) -> List[InstanceResult]:\n",
        "        directory_path = Path(directory)\n",
        "        instance_files = sorted([\n",
        "            f for f in directory_path.glob(pattern)\n",
        "            if f.name.startswith(('TRAIN', 'TEST'))\n",
        "        ])\n",
        "\n",
        "        total = len(instance_files)\n",
        "        self.logger.info(f\"Found {total} instances in {directory}\")\n",
        "\n",
        "        if parallel:\n",
        "            return self._execute_parallel(instance_files, capacity, compute_optimal, max_workers)\n",
        "        else:\n",
        "            return self._execute_sequential(instance_files, capacity, compute_optimal)\n",
        "\n",
        "    def _execute_sequential(self, instance_files: List[Path], capacity: int,\n",
        "                           compute_optimal: bool) -> List[InstanceResult]:\n",
        "        results = []\n",
        "        total = len(instance_files)\n",
        "\n",
        "        for idx, filepath in enumerate(instance_files, 1):\n",
        "            try:\n",
        "                instance = KnapsackInstance.from_csv(str(filepath), capacity)\n",
        "                result = self.execute_single_instance(instance, compute_optimal)\n",
        "                results.append(result)\n",
        "\n",
        "                if idx % 10 == 0:\n",
        "                    self.logger.info(f\"Processed {idx}/{total}\")\n",
        "                    print(f\"Processed {idx}/{total}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Failed to process {filepath}: {e}\")\n",
        "\n",
        "        self.logger.info(f\"Completed: {len(results)}/{total} instances\")\n",
        "        return results\n",
        "\n",
        "    def _execute_parallel(self, instance_files: List[Path], capacity: int,\n",
        "                         compute_optimal: bool, max_workers: Optional[int] = None) -> List[InstanceResult]:\n",
        "        total = len(instance_files)\n",
        "\n",
        "        if max_workers is None:\n",
        "            max_workers = min(32, (psutil.cpu_count(logical=True) or 1) + 4)\n",
        "\n",
        "        self.logger.info(f\"Starting parallel execution with {max_workers} workers\")\n",
        "        print(f\"Processing {total} instances in parallel ({max_workers} workers)...\")\n",
        "\n",
        "        results = []\n",
        "        completed = 0\n",
        "\n",
        "        process_func = partial(\n",
        "            _process_instance_wrapper,\n",
        "            capacity=capacity,\n",
        "            compute_optimal=compute_optimal,\n",
        "            registry=self.registry,\n",
        "            solver=self.optimal_solver\n",
        "        )\n",
        "\n",
        "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "            future_to_filepath = {\n",
        "                executor.submit(process_func, str(filepath)): filepath\n",
        "                for filepath in instance_files\n",
        "            }\n",
        "\n",
        "            for future in as_completed(future_to_filepath):\n",
        "                filepath = future_to_filepath[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    if result is not None:\n",
        "                        results.append(result)\n",
        "                    completed += 1\n",
        "\n",
        "                    if completed % 10 == 0:\n",
        "                        self.logger.info(f\"Completed {completed}/{total}\")\n",
        "                        print(f\"Completed {completed}/{total}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Exception processing {filepath}: {e}\")\n",
        "                    completed += 1\n",
        "\n",
        "        results.sort(key=lambda r: r.instance_name)\n",
        "\n",
        "        self.logger.info(f\"Parallel execution completed: {len(results)}/{total} instances\")\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4YCHVvaYPj-"
      },
      "source": [
        "## 8. Results Converter and Statistical Analyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tYL1R90xYPj-"
      },
      "outputs": [],
      "source": [
        "class ResultsConverter:\n",
        "    @staticmethod\n",
        "    def to_dataframe(results: List[InstanceResult]) -> pd.DataFrame:\n",
        "        records = [result.to_dict() for result in results]\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_execution_times(results: List[InstanceResult]) -> Dict[str, List[float]]:\n",
        "        execution_times = {}\n",
        "\n",
        "        for result in results:\n",
        "            for name, heur_result in result.heuristic_results.items():\n",
        "                if name not in execution_times:\n",
        "                    execution_times[name] = []\n",
        "                execution_times[name].append(heur_result.execution_time)\n",
        "\n",
        "            if result.optimal_time is not None:\n",
        "                if 'optimal' not in execution_times:\n",
        "                    execution_times['optimal'] = []\n",
        "                execution_times['optimal'].append(result.optimal_time)\n",
        "\n",
        "        return execution_times\n",
        "\n",
        "\n",
        "class StatisticalAnalyzer:\n",
        "    \"\"\"\n",
        "    - BASIC_HEURISTICS: Contains the 4 basic heuristics without tiebreak mechanisms\n",
        "    - TIEBREAK_HEURISTICS: Despite the name, this includes SOME heuristics WITHOUT\n",
        "      tiebreak ('default', 'max_profit_per_weight') alongside actual tiebreak heuristics.\n",
        "\n",
        "    The \"TIEBREAK_HEURISTICS\" is the original design for specific comparison purposes with previous works.\n",
        "    \"\"\"\n",
        "    BASIC_HEURISTICS = ['default', 'max_profit', 'max_profit_per_weight', 'min_weight']\n",
        "    TIEBREAK_HEURISTICS = ['default', 'max_profit_tiebreak_weight',\n",
        "                          'min_weight_tiebreak_profit', 'max_profit_per_weight']\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_statistics(df: pd.DataFrame, heuristic_columns: List[str]) -> pd.DataFrame:\n",
        "        stats = []\n",
        "        for col in heuristic_columns:\n",
        "            values = df[col].dropna()\n",
        "            stats.append({\n",
        "                'heuristic': col,\n",
        "                'mean': values.mean(),\n",
        "                'std': values.std(),\n",
        "                'min': values.min(),\n",
        "                'max': values.max(),\n",
        "                'median': values.median()\n",
        "            })\n",
        "        return pd.DataFrame(stats)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_gap_statistics(df: pd.DataFrame, heuristic_columns: List[str],\n",
        "                              optimal_column: str = 'optimal') -> pd.DataFrame:\n",
        "        if optimal_column not in df.columns:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        gap_stats = []\n",
        "        for col in heuristic_columns:\n",
        "            gaps = ((df[optimal_column] - df[col]) / df[optimal_column] * 100)\n",
        "            gap_stats.append({\n",
        "                'heuristic': col,\n",
        "                'mean_gap': gaps.mean(),\n",
        "                'std_gap': gaps.std(),\n",
        "                'min_gap': gaps.min(),\n",
        "                'max_gap': gaps.max(),\n",
        "                'median_gap': gaps.median(),\n",
        "                'optimal_found': (gaps == 0).sum()\n",
        "            })\n",
        "        return pd.DataFrame(gap_stats)\n",
        "\n",
        "    @staticmethod\n",
        "    def identify_dominant_heuristic(df: pd.DataFrame,\n",
        "                                   heuristic_columns: List[str]) -> pd.Series:\n",
        "        best_heuristics = df[heuristic_columns].idxmax(axis=1)\n",
        "        return best_heuristics.value_counts()\n",
        "\n",
        "    @staticmethod\n",
        "    def identify_dominant_basic_heuristics(df: pd.DataFrame) -> pd.Series:\n",
        "        available_basic = [col for col in StatisticalAnalyzer.BASIC_HEURISTICS\n",
        "                          if col in df.columns]\n",
        "        if not available_basic:\n",
        "            return pd.Series(dtype='int64')\n",
        "        best_heuristics = df[available_basic].idxmax(axis=1)\n",
        "        return best_heuristics.value_counts()\n",
        "\n",
        "    @staticmethod\n",
        "    def identify_dominant_tiebreak_heuristics(df: pd.DataFrame) -> pd.Series:\n",
        "        available_tiebreak = [col for col in StatisticalAnalyzer.TIEBREAK_HEURISTICS\n",
        "                             if col in df.columns]\n",
        "        if not available_tiebreak:\n",
        "            return pd.Series(dtype='int64')\n",
        "        best_heuristics = df[available_tiebreak].idxmax(axis=1)\n",
        "        return best_heuristics.value_counts()\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_tiebreak_gap_analysis(df: pd.DataFrame) -> List[dict]:\n",
        "        if 'optimal' not in df.columns:\n",
        "            return []\n",
        "\n",
        "        tiebreak_heuristics = StatisticalAnalyzer.TIEBREAK_HEURISTICS\n",
        "        tiebreak_available = [col for col in tiebreak_heuristics if col in df.columns]\n",
        "\n",
        "        if not tiebreak_available:\n",
        "            return []\n",
        "\n",
        "        tiebreak_instance_counts = {}\n",
        "        for instance in df['instance']:\n",
        "            best_tiebreak = df.loc[df['instance'] == instance, tiebreak_available].idxmax(axis=1).values[0]\n",
        "            tiebreak_instance_counts.setdefault(best_tiebreak, []).append(instance)\n",
        "\n",
        "        tiebreak_gap_stats_dominated = []\n",
        "        for heuristic in tiebreak_available:\n",
        "            if heuristic in tiebreak_instance_counts:\n",
        "                instances = tiebreak_instance_counts[heuristic]\n",
        "                if instances:\n",
        "                    gaps = []\n",
        "                    for instance in instances:\n",
        "                        optimal_val = df.loc[df['instance'] == instance, 'optimal'].values[0]\n",
        "                        heuristic_val = df.loc[df['instance'] == instance, heuristic].values[0]\n",
        "\n",
        "                        if optimal_val > 0:\n",
        "                            gap = ((optimal_val - heuristic_val) / optimal_val) * 100\n",
        "                            gaps.append(gap)\n",
        "\n",
        "                    if gaps:\n",
        "                        tiebreak_gap_stats_dominated.append({\n",
        "                            'heuristic': heuristic,\n",
        "                            'instances': len(instances),\n",
        "                            'mean_gap': np.mean(gaps),\n",
        "                            'std_gap': np.std(gaps) if len(gaps) > 1 else 0,\n",
        "                            'min_gap': np.min(gaps),\n",
        "                            'max_gap': np.max(gaps),\n",
        "                            'optimal_found': sum(1 for g in gaps if g == 0)\n",
        "                        })\n",
        "\n",
        "        return tiebreak_gap_stats_dominated\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_comparison(df: pd.DataFrame, heuristic_columns: List[str]) -> pd.DataFrame:\n",
        "        comparison = df[['instance'] + heuristic_columns].copy()\n",
        "        comparison['best_heuristic'] = comparison[heuristic_columns].idxmax(axis=1)\n",
        "        comparison['best_value'] = comparison[heuristic_columns].max(axis=1)\n",
        "        comparison['worst_value'] = comparison[heuristic_columns].min(axis=1)\n",
        "        comparison['value_range'] = comparison['best_value'] - comparison['worst_value']\n",
        "        return comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aah8Fu2YPj_"
      },
      "source": [
        "## 9. Report Formatter and Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WBTKIetVYPj_"
      },
      "outputs": [],
      "source": [
        "class ReportFormatter:\n",
        "    def __init__(self):\n",
        "        self.mty_tz = pytz.timezone('America/Monterrey')\n",
        "        self.current_time = datetime.now(self.mty_tz)\n",
        "\n",
        "    def format_header(self, title: str, has_optimal: bool = False) -> List[str]:\n",
        "        full_title = title\n",
        "        if has_optimal:\n",
        "            full_title += \" (WITH OPTIMAL)\"\n",
        "\n",
        "        return [\n",
        "            \"=\" * 80,\n",
        "            full_title,\n",
        "            \"=\" * 80\n",
        "        ]\n",
        "\n",
        "    def format_section(self, title: str) -> List[str]:\n",
        "        return [\"\", title.upper(), \"-\" * 80]\n",
        "\n",
        "    def format_execution_info(self) -> List[str]:\n",
        "        return [\n",
        "            f\"Date: {self.current_time.strftime('%Y-%m-%d')}\",\n",
        "            f\"Time: {self.current_time.strftime('%H:%M:%S')}\",\n",
        "            f\"Timezone: America/Monterrey (UTC{self.current_time.strftime('%z')})\"\n",
        "        ]\n",
        "\n",
        "    def format_hardware_info(self) -> List[str]:\n",
        "        lines = []\n",
        "\n",
        "        try:\n",
        "            cpu_count = psutil.cpu_count(logical=True)\n",
        "            cpu_count_physical = psutil.cpu_count(logical=False)\n",
        "            memory = psutil.virtual_memory()\n",
        "            cpu_freq = psutil.cpu_freq()\n",
        "\n",
        "            lines.extend([\n",
        "                f\"CPU Cores: {cpu_count_physical} physical, {cpu_count} logical\",\n",
        "                f\"Total Memory: {memory.total / (1024**3):.1f} GB\",\n",
        "                f\"Available Memory: {memory.available / (1024**3):.1f} GB\",\n",
        "                f\"Memory Usage: {memory.percent}%\"\n",
        "            ])\n",
        "\n",
        "            if cpu_freq:\n",
        "                lines.append(f\"CPU Frequency: {cpu_freq.current:.0f} MHz\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not retrieve basic hardware info: {e}\")\n",
        "\n",
        "        if HAS_CPUINFO:\n",
        "            try:\n",
        "                cpu_info = cpuinfo.get_cpu_info()\n",
        "                brand = cpu_info.get('brand_raw', cpu_info.get('brand', 'Unknown'))\n",
        "                lines.append(f\"CPU Model: {brand}\")\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"cpuinfo failed, skipping detailed CPU info: {e}\")\n",
        "        else:\n",
        "            lines.append(\"CPU Model: Install 'py-cpuinfo' for detailed CPU information\")\n",
        "\n",
        "        return lines\n",
        "\n",
        "    def format_statistics_table(self, stats_df: pd.DataFrame) -> List[str]:\n",
        "        lines = []\n",
        "        header = f\"{'Heuristic':<40} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\"\n",
        "        lines.append(header)\n",
        "        lines.append(\"-\" * 80)\n",
        "\n",
        "        for _, row in stats_df.iterrows():\n",
        "            if row['heuristic'] != 'optimal':\n",
        "                lines.append(\n",
        "                    f\"{row['heuristic']:<40} \"\n",
        "                    f\"{row['mean']:>10.2f} \"\n",
        "                    f\"{row['std']:>10.2f} \"\n",
        "                    f\"{row['min']:>10.0f} \"\n",
        "                    f\"{row['max']:>10.0f}\"\n",
        "                )\n",
        "        return lines\n",
        "\n",
        "    def format_gap_table(self, gap_stats_df: pd.DataFrame, total_instances: int) -> List[str]:\n",
        "        lines = []\n",
        "        header = (f\"{'Heuristic':<40} {'Mean Gap %':>12} {'Std Gap %':>12} \"\n",
        "                 f\"{'Min Gap %':>12} {'Max Gap %':>12} {'Optimal Found':>15}\")\n",
        "        lines.append(header)\n",
        "        lines.append(\"-\" * 130)\n",
        "\n",
        "        sorted_stats = gap_stats_df.sort_values('mean_gap')\n",
        "        for _, row in sorted_stats.iterrows():\n",
        "            lines.append(\n",
        "                f\"{row['heuristic']:<40} \"\n",
        "                f\"{row['mean_gap']:>12.2f} \"\n",
        "                f\"{row['std_gap']:>12.2f} \"\n",
        "                f\"{row['min_gap']:>12.2f} \"\n",
        "                f\"{row['max_gap']:>12.2f} \"\n",
        "                f\"{row['optimal_found']:>8}/{total_instances}\"\n",
        "            )\n",
        "\n",
        "        best_heuristic = sorted_stats.iloc[0]['heuristic']\n",
        "        best_gap = sorted_stats.iloc[0]['mean_gap']\n",
        "        worst_heuristic = sorted_stats.iloc[-1]['heuristic']\n",
        "        worst_gap = sorted_stats.iloc[-1]['mean_gap']\n",
        "\n",
        "        lines.append(\"\")\n",
        "        lines.append(f\"Best gap: {best_heuristic} ({best_gap:.2f}% avg)\")\n",
        "        lines.append(f\"Worst gap: {worst_heuristic} ({worst_gap:.2f}% avg)\")\n",
        "\n",
        "        return lines\n",
        "\n",
        "    def format_tiebreak_gap_table(self, tiebreak_stats: List[dict]) -> List[str]:\n",
        "        if not tiebreak_stats:\n",
        "            return [\"No tie-break dominance data available\"]\n",
        "\n",
        "        lines = []\n",
        "        header = (f\"{'Heuristic':<40} {'Instances':>10} {'Mean Gap %':>12} {'Std Gap %':>12} \"\n",
        "                 f\"{'Min Gap %':>12} {'Max Gap %':>12} {'Optimal Found':>15}\")\n",
        "        lines.append(header)\n",
        "        lines.append(\"-\" * 140)\n",
        "\n",
        "        sorted_stats = sorted(tiebreak_stats, key=lambda x: x['mean_gap'])\n",
        "        for stats in sorted_stats:\n",
        "            lines.append(\n",
        "                f\"{stats['heuristic']:<40} \"\n",
        "                f\"{stats['instances']:>10} \"\n",
        "                f\"{stats['mean_gap']:>12.2f} \"\n",
        "                f\"{stats['std_gap']:>12.2f} \"\n",
        "                f\"{stats['min_gap']:>12.2f} \"\n",
        "                f\"{stats['max_gap']:>12.2f} \"\n",
        "                f\"{stats['optimal_found']:>8}/{stats['instances']}\"\n",
        "            )\n",
        "\n",
        "        if sorted_stats:\n",
        "            best = sorted_stats[0]\n",
        "            worst = sorted_stats[-1]\n",
        "            lines.append(\"\")\n",
        "            lines.append(f\"Best tie-break gap (when dominated): {best['heuristic']} ({best['mean_gap']:.2f}% avg, {best['instances']} instances)\")\n",
        "            lines.append(f\"Worst tie-break gap (when dominated): {worst['heuristic']} ({worst['mean_gap']:.2f}% avg, {worst['instances']} instances)\")\n",
        "\n",
        "        return lines\n",
        "\n",
        "    def format_dominant_heuristics(self, dominant: pd.Series, total: int, category: str = \"ALL\") -> List[str]:\n",
        "        if dominant.empty:\n",
        "            return [f\"No dominance data for {category} heuristics\"]\n",
        "\n",
        "        lines = [f\"Number of times each {category} heuristic achieved best solution:\", \"\"]\n",
        "        for heuristic, count in dominant.sort_values(ascending=False).items():\n",
        "            percentage = (count / total) * 100\n",
        "            lines.append(f\"  {heuristic:<40} {count:>6} instances ({percentage:>5.1f}%)\")\n",
        "\n",
        "        return lines\n",
        "\n",
        "    def format_execution_times(self, avg_times: Dict[str, float],\n",
        "                              total_times: Dict[str, float]) -> List[str]:\n",
        "        lines = []\n",
        "        header = f\"{'Method':<40} {'Avg/Inst (Î¼s)':>15} {'Total (ms)':>15}\"\n",
        "        lines.append(header)\n",
        "        lines.append(\"-\" * 80)\n",
        "\n",
        "        for name in sorted(avg_times.keys()):\n",
        "            if name != 'optimal':\n",
        "                avg_us = avg_times[name] * 1_000_000\n",
        "                total_ms = total_times.get(name, 0) * 1000\n",
        "                lines.append(f\"{name:<40} {avg_us:>15.3f} {total_ms:>15.3f}\")\n",
        "\n",
        "        if 'optimal' in avg_times:\n",
        "            avg_us = avg_times['optimal'] * 1_000_000\n",
        "            total_ms = total_times.get('optimal', 0) * 1000\n",
        "            lines.append(f\"{'DP Optimal Solver':<40} {avg_us:>15.3f} {total_ms:>15.3f}\")\n",
        "\n",
        "        return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vB8Y_0vYYPj_"
      },
      "outputs": [],
      "source": [
        "class ReportGenerator:\n",
        "    def __init__(self, formatter: ReportFormatter, analyzer: StatisticalAnalyzer):\n",
        "        self.formatter = formatter\n",
        "        self.analyzer = analyzer\n",
        "        self.logger = logging.getLogger(__name__ + '.ReportGenerator')\n",
        "\n",
        "    def generate(self, df: pd.DataFrame, execution_times: Dict[str, List[float]],\n",
        "                config: dict, output_path: str) -> str:\n",
        "        lines = []\n",
        "        heuristic_columns = [col for col in df.columns\n",
        "                           if col not in ['instance', 'capacity', 'optimal']]\n",
        "\n",
        "        has_optimal = 'optimal' in df.columns\n",
        "\n",
        "        lines.extend(self.formatter.format_header(\n",
        "            \"KNAPSACK HEURISTICS EXPERIMENT REPORT\", has_optimal))\n",
        "\n",
        "        lines.extend(self.formatter.format_section(\"EXECUTION INFORMATION\"))\n",
        "        lines.extend(self.formatter.format_execution_info())\n",
        "\n",
        "        lines.extend(self.formatter.format_section(\"HARDWARE INFORMATION\"))\n",
        "        lines.extend(self.formatter.format_hardware_info())\n",
        "\n",
        "        lines.extend(self.formatter.format_section(\"EXPERIMENT CONFIGURATION\"))\n",
        "        lines.extend(self._format_config(config, df))\n",
        "\n",
        "        lines.extend(self.formatter.format_section(\"HEURISTICS EVALUATED\"))\n",
        "        lines.extend([f\"{i}. {h}\" for i, h in enumerate(heuristic_columns, 1)])\n",
        "\n",
        "        stats_df = self.analyzer.compute_statistics(df, heuristic_columns)\n",
        "        lines.extend(self.formatter.format_section(\"HEURISTIC PERFORMANCE STATISTICS\"))\n",
        "        lines.extend(self.formatter.format_statistics_table(stats_df))\n",
        "\n",
        "        if has_optimal:\n",
        "            gap_stats = self.analyzer.compute_gap_statistics(df, heuristic_columns)\n",
        "            lines.extend(self.formatter.format_section(\"OPTIMALITY GAP ANALYSIS\"))\n",
        "            lines.extend(self.formatter.format_gap_table(gap_stats, len(df)))\n",
        "\n",
        "        avg_times = {name: np.mean(times) for name, times in execution_times.items()}\n",
        "        total_times = {name: np.sum(times) for name, times in execution_times.items()}\n",
        "        lines.extend(self.formatter.format_section(\"EXECUTION TIME ANALYSIS\"))\n",
        "        lines.extend(self.formatter.format_execution_times(avg_times, total_times))\n",
        "\n",
        "        dominant = self.analyzer.identify_dominant_heuristic(df, heuristic_columns)\n",
        "        lines.extend(self.formatter.format_section(\"DOMINANT HEURISTIC ANALYSIS (ALL HEURISTICS)\"))\n",
        "        lines.extend(self.formatter.format_dominant_heuristics(dominant, len(df), \"ALL\"))\n",
        "\n",
        "        dominant_basic = self.analyzer.identify_dominant_basic_heuristics(df)\n",
        "        if not dominant_basic.empty:\n",
        "            lines.extend(self.formatter.format_section(\"BASIC HEURISTICS DOMINANCE ANALYSIS\"))\n",
        "            lines.extend(self.formatter.format_dominant_heuristics(dominant_basic, len(df), \"BASIC\"))\n",
        "\n",
        "        dominant_tiebreak = self.analyzer.identify_dominant_tiebreak_heuristics(df)\n",
        "        if not dominant_tiebreak.empty:\n",
        "            lines.extend(self.formatter.format_section(\"TIE-BREAK HEURISTICS DOMINANCE ANALYSIS\"))\n",
        "            lines.extend(self.formatter.format_dominant_heuristics(dominant_tiebreak, len(df), \"TIE-BREAK\"))\n",
        "\n",
        "        if has_optimal:\n",
        "            tiebreak_gap_stats = self.analyzer.compute_tiebreak_gap_analysis(df)\n",
        "            if tiebreak_gap_stats:\n",
        "                lines.extend(self.formatter.format_section(\"TIE-BREAK OPTIMALITY GAP ANALYSIS (Instances where each dominated)\"))\n",
        "                lines.extend(self.formatter.format_tiebreak_gap_table(tiebreak_gap_stats))\n",
        "\n",
        "        lines.extend([\"\", \"END OF REPORT\", \"=\" * 80])\n",
        "\n",
        "        report_text = '\\n'.join(lines)\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(report_text)\n",
        "\n",
        "        self.logger.info(f\"Report generated: {output_path}\")\n",
        "        return report_text\n",
        "\n",
        "    def _format_config(self, config: dict, df: pd.DataFrame) -> List[str]:\n",
        "        return [\n",
        "            f\"Capacity: {config.get('capacity', 'N/A')}\",\n",
        "            f\"Data directory: {config.get('data_dir', 'N/A')}\",\n",
        "            f\"Total instances: {len(df)}\",\n",
        "            f\"Compute optimal: {'optimal' in df.columns}\",\n",
        "            f\"Parallel processing: {config.get('enable_parallel', False)}\",\n",
        "            f\"Python version: {config.get('python_version', 'N/A')}\",\n",
        "            f\"NumPy version: {np.__version__}\",\n",
        "            f\"Pandas version: {pd.__version__}\"\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOqIrL-RYPj_"
      },
      "source": [
        "## 10. Utility Functions (I/O)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CTvAbmXtYPkA"
      },
      "outputs": [],
      "source": [
        "def save_results(df: pd.DataFrame, output_path: str, include_timestamp: bool = True,\n",
        "                base_name: Optional[str] = None) -> str:\n",
        "    if include_timestamp:\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        path_obj = Path(output_path)\n",
        "\n",
        "        if base_name:\n",
        "            filename = f\"{base_name}_{timestamp}.csv\"\n",
        "        else:\n",
        "            filename = f\"results_{timestamp}.csv\"\n",
        "\n",
        "        if path_obj.is_dir():\n",
        "            final_path = path_obj / filename\n",
        "        else:\n",
        "            final_path = path_obj.parent / f\"{path_obj.stem}_{timestamp}{path_obj.suffix}\"\n",
        "    else:\n",
        "        final_path = Path(output_path)\n",
        "\n",
        "    df.to_csv(final_path, index=False)\n",
        "    logger.info(f\"Saved results: {final_path}\")\n",
        "    return str(final_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSEpl37vYPkA"
      },
      "source": [
        "## 11. Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uInoKIYYPkA",
        "outputId": "e7271090-48bd-4243-9a3d-39b383010e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run tests with: unittest.main(argv=[''], exit=False, verbosity=2)\n"
          ]
        }
      ],
      "source": [
        "import unittest\n",
        "import tempfile\n",
        "\n",
        "class TestKnapsackInstance(unittest.TestCase):\n",
        "    def test_valid_instance_creation(self):\n",
        "        instance = KnapsackInstance(\n",
        "            profits=np.array([10, 20, 30], dtype=np.int64),\n",
        "            weights=np.array([5, 10, 15], dtype=np.int64),\n",
        "            capacity=20,\n",
        "            name='test'\n",
        "        )\n",
        "        self.assertEqual(instance.num_items, 3)\n",
        "        self.assertEqual(instance.capacity, 20)\n",
        "\n",
        "    def test_negative_capacity_raises_error(self):\n",
        "        from pydantic import ValidationError\n",
        "        with self.assertRaises(ValidationError):\n",
        "            KnapsackInstance(\n",
        "                profits=np.array([10, 20], dtype=np.int64),\n",
        "                weights=np.array([5, 10], dtype=np.int64),\n",
        "                capacity=-5,\n",
        "                name='invalid'\n",
        "            )\n",
        "\n",
        "\n",
        "class TestOptimalSolver(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.solver = OptimalSolver()\n",
        "\n",
        "    def test_known_optimal_solution(self):\n",
        "        instance = KnapsackInstance(\n",
        "            profits=np.array([60, 100, 120], dtype=np.int64),\n",
        "            weights=np.array([10, 20, 30], dtype=np.int64),\n",
        "            capacity=50,\n",
        "            name='test'\n",
        "        )\n",
        "        result = self.solver.solve(instance)\n",
        "        self.assertEqual(result, 220)\n",
        "\n",
        "\n",
        "class TestHeuristics(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.instance = KnapsackInstance(\n",
        "            profits=np.array([60, 100, 120], dtype=np.int64),\n",
        "            weights=np.array([10, 20, 30], dtype=np.int64),\n",
        "            capacity=50,\n",
        "            name='test'\n",
        "        )\n",
        "        self.optimal_value = 220\n",
        "\n",
        "    def test_all_heuristics_deterministic(self):\n",
        "        heuristics = [\n",
        "            MaxProfitHeuristic(),\n",
        "            MaxProfitPerWeightHeuristic(),\n",
        "            MinWeightHeuristic()\n",
        "        ]\n",
        "\n",
        "        for heuristic in heuristics:\n",
        "            result1 = heuristic.solve(self.instance)\n",
        "            result2 = heuristic.solve(self.instance)\n",
        "            self.assertEqual(result1, result2,\n",
        "                           f\"{heuristic.name} is not deterministic\")\n",
        "\n",
        "    def test_heuristics_do_not_exceed_optimal(self):\n",
        "        heuristics = [\n",
        "            MaxProfitHeuristic(),\n",
        "            MaxProfitPerWeightHeuristic(),\n",
        "            MinWeightHeuristic()\n",
        "        ]\n",
        "\n",
        "        for heuristic in heuristics:\n",
        "            result = heuristic.solve(self.instance)\n",
        "            self.assertLessEqual(result, self.optimal_value,\n",
        "                f\"{heuristic.name} exceeded optimal value\")\n",
        "\n",
        "print(\"\\nRun tests with: unittest.main(argv=[''], exit=False, verbosity=2)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJyyaivMYPkA"
      },
      "source": [
        "### Run Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4JW8J_dYPkA",
        "outputId": "33017820-8940-487f-c71b-d36013deb73a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_all_heuristics_deterministic (__main__.TestHeuristics.test_all_heuristics_deterministic) ... ok\n",
            "test_heuristics_do_not_exceed_optimal (__main__.TestHeuristics.test_heuristics_do_not_exceed_optimal) ... ok\n",
            "test_negative_capacity_raises_error (__main__.TestKnapsackInstance.test_negative_capacity_raises_error) ... ok\n",
            "test_valid_instance_creation (__main__.TestKnapsackInstance.test_valid_instance_creation) ... ok\n",
            "test_known_optimal_solution (__main__.TestOptimalSolver.test_known_optimal_solution) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.485s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e245d25eb10>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Run all unit tests\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eckCVtLiYPkA"
      },
      "source": [
        "## 12. Main Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwB3c2PJYPkA"
      },
      "source": [
        "### 12.1 Initialize Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9gVOPCPYPkA",
        "outputId": "d9ecb9e5-cb81-4047-a882-cad0f177658e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Starting Knapsack Heuristics Experiment\n",
            "INFO:__main__:Configuration: {'capacity': 64, 'data_dir': '/content/drive/MyDrive/PlataZarateBayliss-Instances_extracted_csv', 'output_dir': '/content', 'pattern': '*.csv', 'compute_optimal': True, 'timezone': 'America/Monterrey', 'log_level': 'INFO', 'random_seed': None, 'max_workers': None, 'python_version': '3.12.12', 'numpy_version': '2.0.2', 'pandas_version': '2.2.2', 'timestamp': '2025-12-22T17:28:58.914188-06:00'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "EXPERIMENT CONFIGURATION\n",
            "================================================================================\n",
            "capacity                 : 64\n",
            "data_dir                 : /content/drive/MyDrive/PlataZarateBayliss-Instances_extracted_csv\n",
            "output_dir               : /content\n",
            "pattern                  : *.csv\n",
            "compute_optimal          : True\n",
            "timezone                 : America/Monterrey\n",
            "log_level                : INFO\n",
            "random_seed              : None\n",
            "max_workers              : None\n",
            "python_version           : 3.12.12\n",
            "numpy_version            : 2.0.2\n",
            "pandas_version           : 2.2.2\n",
            "timestamp                : 2025-12-22T17:28:58.916033-06:00\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"Starting Knapsack Heuristics Experiment\")\n",
        "\n",
        "config = ExperimentConfig()\n",
        "config.setup_logging()\n",
        "\n",
        "logger.info(f\"Configuration: {config.to_dict()}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "for key, value in config.to_dict().items():\n",
        "    print(f\"{key:<25}: {value}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPMSpnlCYPkA"
      },
      "source": [
        "### 12.2 Mount Google Drive (Colab only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mRq0Ny4YPkA",
        "outputId": "b99657c1-bf53-400c-c818-7121ad67702f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted\")\n",
        "else:\n",
        "    print(\"Not running in Colab, skipping Drive mount\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRTkb3oQYPkA"
      },
      "source": [
        "### 12.3 Initialize Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBqk7iIHYPkA",
        "outputId": "03eac9df-8bec-4b8f-9731-66468ea6183d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Initialized with 8 heuristics:\n",
            "  1. default\n",
            "  2. max_profit\n",
            "  3. max_profit_per_weight\n",
            "  4. min_weight\n",
            "  5. max_profit_per_weight_tiebreak_profit\n",
            "  6. max_profit_per_weight_tiebreak_weight\n",
            "  7. max_profit_tiebreak_weight\n",
            "  8. min_weight_tiebreak_profit\n"
          ]
        }
      ],
      "source": [
        "csv_dir = config.data_dir\n",
        "CAPACITY = config.capacity\n",
        "\n",
        "registry = HeuristicRegistry()\n",
        "optimal_solver = OptimalSolver()\n",
        "executor = ExperimentExecutor(registry, optimal_solver)\n",
        "\n",
        "print(f\"\\n Initialized with {len(registry.get_names())} heuristics:\")\n",
        "for i, name in enumerate(registry.get_names(), 1):\n",
        "    print(f\"  {i}. {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIxd_JtfYPkA"
      },
      "source": [
        "### 12.4 Process All Instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVsZNLuxYPkA",
        "outputId": "1ba47703-b363-41c3-be64-0e398a11f2d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Processing instances from: /content/drive/MyDrive/PlataZarateBayliss-Instances_extracted_csv\n",
            "INFO:__main__:Parallel processing: True, Max workers: auto\n",
            "INFO:__main__.ExperimentExecutor:Found 500 instances in /content/drive/MyDrive/PlataZarateBayliss-Instances_extracted_csv\n",
            "INFO:__main__.ExperimentExecutor:Starting parallel execution with 6 workers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 500 instances in parallel (6 workers)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 10/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 10/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 20/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 30/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 40/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 50/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 20/500\n",
            "Completed 30/500\n",
            "Completed 40/500\n",
            "Completed 50/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 60/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 70/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 80/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 90/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 60/500\n",
            "Completed 70/500\n",
            "Completed 80/500\n",
            "Completed 90/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 100/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 110/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 120/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 130/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 140/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 100/500\n",
            "Completed 110/500\n",
            "Completed 120/500\n",
            "Completed 130/500\n",
            "Completed 140/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 150/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 160/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 170/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 180/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 190/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 150/500\n",
            "Completed 160/500\n",
            "Completed 170/500\n",
            "Completed 180/500\n",
            "Completed 190/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 200/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 210/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 220/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 230/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 240/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 200/500\n",
            "Completed 210/500\n",
            "Completed 220/500\n",
            "Completed 230/500\n",
            "Completed 240/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 250/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 260/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 270/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 280/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 290/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 250/500\n",
            "Completed 260/500\n",
            "Completed 270/500\n",
            "Completed 280/500\n",
            "Completed 290/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 300/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 310/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 320/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 300/500\n",
            "Completed 310/500\n",
            "Completed 320/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 330/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 340/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 350/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 360/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 370/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 330/500\n",
            "Completed 340/500\n",
            "Completed 350/500\n",
            "Completed 360/500\n",
            "Completed 370/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 380/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 390/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 400/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 410/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 420/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 380/500\n",
            "Completed 390/500\n",
            "Completed 400/500\n",
            "Completed 410/500\n",
            "Completed 420/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 430/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 440/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 450/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 460/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 470/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 430/500\n",
            "Completed 440/500\n",
            "Completed 450/500\n",
            "Completed 460/500\n",
            "Completed 470/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__.ExperimentExecutor:Completed 480/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 490/500\n",
            "INFO:__main__.ExperimentExecutor:Completed 500/500\n",
            "INFO:__main__.ExperimentExecutor:Parallel execution completed: 500/500 instances\n",
            "INFO:__main__:Processed 500 instances\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 480/500\n",
            "Completed 490/500\n",
            "Completed 500/500\n"
          ]
        }
      ],
      "source": [
        "logger.info(f\"Processing instances from: {csv_dir}\")\n",
        "logger.info(f\"Parallel processing: {config.enable_parallel}, Max workers: {config.max_workers or 'auto'}\")\n",
        "\n",
        "results = executor.execute_directory(\n",
        "    directory=csv_dir,\n",
        "    capacity=CAPACITY,\n",
        "    pattern=config.pattern,\n",
        "    compute_optimal=config.compute_optimal,\n",
        "    parallel=config.enable_parallel,\n",
        "    max_workers=config.max_workers\n",
        ")\n",
        "\n",
        "logger.info(f\"Processed {len(results)} instances\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC7SfYDKYPkB"
      },
      "source": [
        "### 12.5 Convert Results to DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvZBhprKYPkB",
        "outputId": "06136851-bd8c-401a-fe1f-2e19d41362ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First few rows:\n",
            "                   instance  capacity  default  max_profit  \\\n",
            "0  TEST_GA-DEF_EASY_100_000        64      686         612   \n",
            "1  TEST_GA-DEF_EASY_100_001        64      704         311   \n",
            "2  TEST_GA-DEF_EASY_100_002        64      571         272   \n",
            "3  TEST_GA-DEF_EASY_100_003        64     1046         711   \n",
            "4  TEST_GA-DEF_EASY_100_004        64      691         352   \n",
            "\n",
            "   max_profit_per_weight  min_weight  max_profit_per_weight_tiebreak_profit  \\\n",
            "0                    613         613                                    613   \n",
            "1                    643         507                                    643   \n",
            "2                    513         486                                    513   \n",
            "3                    975         975                                    975   \n",
            "4                    625         579                                    625   \n",
            "\n",
            "   max_profit_per_weight_tiebreak_weight  max_profit_tiebreak_weight  \\\n",
            "0                                    613                         612   \n",
            "1                                    643                         311   \n",
            "2                                    513                         272   \n",
            "3                                    975                         711   \n",
            "4                                    625                         453   \n",
            "\n",
            "   min_weight_tiebreak_profit  optimal  \n",
            "0                         613      708  \n",
            "1                         507      704  \n",
            "2                         486      571  \n",
            "3                         975     1046  \n",
            "4                         625      691  \n",
            "\n",
            "DataFrame shape: (500, 11)\n",
            "Columns: ['instance', 'capacity', 'default', 'max_profit', 'max_profit_per_weight', 'min_weight', 'max_profit_per_weight_tiebreak_profit', 'max_profit_per_weight_tiebreak_weight', 'max_profit_tiebreak_weight', 'min_weight_tiebreak_profit', 'optimal']\n"
          ]
        }
      ],
      "source": [
        "converter = ResultsConverter()\n",
        "df = converter.to_dataframe(results)\n",
        "execution_times = converter.extract_execution_times(results)\n",
        "\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(f\"\\nDataFrame shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiF608DbYPkB"
      },
      "source": [
        "### 12.6 Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQeblGqmYPkB",
        "outputId": "578942a1-5de3-4d40-8978-6179cd96895b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Saved results: /content/knapsack_results_20251222_232907.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results saved to: /content/knapsack_results_20251222_232907.csv\n"
          ]
        }
      ],
      "source": [
        "output_file = save_results(\n",
        "    df=df,\n",
        "    output_path=Path(config.output_dir) / 'knapsack_results.csv',\n",
        "    include_timestamp=True,\n",
        "    base_name=f'heuristic_results_cap{CAPACITY}'\n",
        ")\n",
        "\n",
        "print(f\"\\nResults saved to: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBk11kIJYPkB"
      },
      "source": [
        "### 12.7 Generate Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YycZ3lg8YPkC",
        "outputId": "39b8317e-2060-445b-b07e-26244143d15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Configuration saved to /content/experiment_config_20251222_172907.json\n",
            "INFO:__main__.ReportGenerator:Report generated: /content/experiment_report_CAP64_20251222_172907.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Report saved to: /content/experiment_report_CAP64_20251222_172907.txt\n",
            "Configuration saved to: /content/experiment_config_20251222_172907.json\n"
          ]
        }
      ],
      "source": [
        "timestamp = datetime.now(pytz.timezone(config.timezone)).strftime('%Y%m%d_%H%M%S')\n",
        "report_path = Path(config.output_dir) / f'experiment_report_CAP{CAPACITY}_{timestamp}.txt'\n",
        "config_path = Path(config.output_dir) / f'experiment_config_{timestamp}.json'\n",
        "\n",
        "config.save_to_file(str(config_path))\n",
        "\n",
        "formatter = ReportFormatter()\n",
        "analyzer = StatisticalAnalyzer()\n",
        "report_gen = ReportGenerator(formatter, analyzer)\n",
        "\n",
        "report_gen.generate(\n",
        "    df=df,\n",
        "    execution_times=execution_times,\n",
        "    config=config.to_dict(),\n",
        "    output_path=str(report_path)\n",
        ")\n",
        "\n",
        "print(f\"\\nReport saved to: {report_path}\")\n",
        "print(f\"Configuration saved to: {config_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y81cRVKYPkC"
      },
      "source": [
        "### 12.8 Display Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROsi3CpNYPkC",
        "outputId": "fcb186e9-1b65-4a88-a571-cc2528ff8153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "EXPERIMENT SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Heuristic Performance Statistics:\n",
            "                            heuristic    mean        std  min  max  median\n",
            "                              default 378.130 239.194191    4 1074   297.5\n",
            "                           max_profit 524.438 241.761203  127 1303   492.5\n",
            "                max_profit_per_weight 892.068 238.096407  394 1476   894.0\n",
            "                           min_weight 696.612 284.167528  184 1498   625.5\n",
            "max_profit_per_weight_tiebreak_profit 892.142 237.544490  394 1476   894.0\n",
            "max_profit_per_weight_tiebreak_weight 892.172 238.451296  394 1476   894.0\n",
            "           max_profit_tiebreak_weight 571.094 239.951175  128 1303   543.0\n",
            "           min_weight_tiebreak_profit 724.798 273.410651  184 1511   655.5\n",
            "\n",
            "Optimality Gap Analysis:\n",
            "                            heuristic  mean_gap  optimal_found\n",
            "                              default 55.508541            101\n",
            "                           max_profit 41.957777            104\n",
            "                max_profit_per_weight  6.641649            107\n",
            "                           min_weight 26.022800             79\n",
            "max_profit_per_weight_tiebreak_profit  6.615770            110\n",
            "max_profit_per_weight_tiebreak_weight  6.638613            109\n",
            "           max_profit_tiebreak_weight 37.185099            115\n",
            "           min_weight_tiebreak_profit 23.339883             98\n",
            "\n",
            "Dominant Heuristic (times achieved best solution):\n",
            "default                                  125\n",
            "max_profit_per_weight                    124\n",
            "max_profit                               113\n",
            "min_weight                                98\n",
            "min_weight_tiebreak_profit                25\n",
            "max_profit_tiebreak_weight                12\n",
            "max_profit_per_weight_tiebreak_profit      3\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "heuristic_columns = [col for col in df.columns if col not in ['instance', 'capacity', 'optimal']]\n",
        "stats_df = analyzer.compute_statistics(df, heuristic_columns)\n",
        "\n",
        "print(\"\\nHeuristic Performance Statistics:\")\n",
        "print(stats_df.to_string(index=False))\n",
        "\n",
        "if 'optimal' in df.columns:\n",
        "    gap_stats = analyzer.compute_gap_statistics(df, heuristic_columns)\n",
        "    print(\"\\nOptimality Gap Analysis:\")\n",
        "    print(gap_stats[['heuristic', 'mean_gap', 'optimal_found']].to_string(index=False))\n",
        "\n",
        "dominant = analyzer.identify_dominant_heuristic(df, heuristic_columns)\n",
        "print(\"\\nDominant Heuristic (times achieved best solution):\")\n",
        "print(dominant.to_string())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datanalysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}